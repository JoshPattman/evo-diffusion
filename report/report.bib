@article{original-paper,
  author   = {Watson, Richard A. and Wagner, Günter P. and Pavlicev, Mihaela and Weinreich, Daniel M. and Mills, Rob},
  title    = {THE EVOLUTION OF PHENOTYPIC CORRELATIONS AND “DEVELOPMENTAL MEMORY”},
  journal  = {Evolution},
  volume   = {68},
  number   = {4},
  pages    = {1124-1138},
  keywords = {Adaptation, associative learning, evolvability, evo-devo},
  doi      = {https://doi.org/10.1111/evo.12337},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/evo.12337},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/evo.12337},
  abstract = {Development introduces structured correlations among traits that may constrain or bias the distribution of phenotypes produced. Moreover, when suitable heritable variation exists, natural selection may alter such constraints and correlations, affecting the phenotypic variation available to subsequent selection. However, exactly how the distribution of phenotypes produced by complex developmental systems can be shaped by past selective environments is poorly understood. Here we investigate the evolution of a network of recurrent nonlinear ontogenetic interactions, such as a gene regulation network, in various selective scenarios. We find that evolved networks of this type can exhibit several phenomena that are familiar in cognitive learning systems. These include formation of a distributed associative memory that can “store” and “recall” multiple phenotypes that have been selected in the past, recreate complete adult phenotypic patterns accurately from partial or corrupted embryonic phenotypes, and “generalize” (by exploiting evolved developmental modules) to produce new combinations of phenotypic features. We show that these surprising behaviors follow from an equivalence between the action of natural selection on phenotypic correlations and associative learning, well-understood in the context of neural networks. This helps to explain how development facilitates the evolution of high-fitness phenotypes and how this ability changes over evolutionary time.},
  year     = {2014}
}

@article{advanced-paper,
  doi       = {10.1371/journal.pcbi.1005358},
  author    = {Kouvaris, Kostas AND Clune, Jeff AND Kounios, Loizos AND Brede, Markus AND Watson, Richard A.},
  journal   = {PLOS Computational Biology},
  publisher = {Public Library of Science},
  title     = {How evolution learns to generalise: Using the principles of learning theory to understand the evolution of developmental organisation},
  year      = {2017},
  month     = {04},
  volume    = {13},
  url       = {https://doi.org/10.1371/journal.pcbi.1005358},
  pages     = {1-20},
  abstract  = {One of the most intriguing questions in evolution is how organisms exhibit suitable phenotypic variation to rapidly adapt in novel selective environments. Such variability is crucial for evolvability, but poorly understood. In particular, how can natural selection favour developmental organisations that facilitate adaptive evolution in previously unseen environments? Such a capacity suggests foresight that is incompatible with the short-sighted concept of natural selection. A potential resolution is provided by the idea that evolution may discover and exploit information not only about the particular phenotypes selected in the past, but their underlying structural regularities: new phenotypes, with the same underlying regularities, but novel particulars, may then be useful in new environments. If true, we still need to understand the conditions in which natural selection will discover such deep regularities rather than exploiting ‘quick fixes’ (i.e., fixes that provide adaptive phenotypes in the short term, but limit future evolvability). Here we argue that the ability of evolution to discover such regularities is formally analogous to learning principles, familiar in humans and machines, that enable generalisation from past experience. Conversely, natural selection that fails to enhance evolvability is directly analogous to the learning problem of over-fitting and the subsequent failure to generalise. We support the conclusion that evolving systems and learning systems are different instantiations of the same algorithmic principles by showing that existing results from the learning domain can be transferred to the evolution domain. Specifically, we show that conditions that alleviate over-fitting in learning systems successfully predict which biological conditions (e.g., environmental variation, regularity, noise or a pressure for developmental simplicity) enhance evolvability. This equivalence provides access to a well-developed theoretical framework from learning theory that enables a characterisation of the general conditions for the evolution of evolvability.},
  number    = {4}
}